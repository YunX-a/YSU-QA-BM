# YSU æ•™åŠ¡æ™ºèƒ½é—®ç­”å¤§æ¨¡å‹ (YSU-QA-BM)

åŸºäº ChatGLM3-6B å’Œ LoRA å¾®è°ƒæŠ€æœ¯ï¼Œä¸“ä¸ºç‡•å±±å¤§å­¦æ•™åŠ¡åœºæ™¯å®šåˆ¶çš„æ™ºèƒ½é—®ç­”åŠ©æ‰‹ã€‚

## ğŸš€ é¡¹ç›®ç®€ä»‹

æœ¬é¡¹ç›®åœ¨ AutoDL ç®—åŠ›äº‘ä¸Šè¿›è¡Œè®­ç»ƒå’Œéƒ¨ç½²ï¼Œä½¿ç”¨ `LLaMA-Factory` è¿›è¡Œå¾®è°ƒï¼Œå¹¶é›†æˆäº† `vLLM` æ¨ç†åŠ é€Ÿä¸ `Streamlit` å‰ç«¯äº¤äº’ç•Œé¢ã€‚

## ğŸ“‚ ç›®å½•ç»“æ„

- `app.py`: Streamlit å‰ç«¯ Web ç•Œé¢ä»£ç 
- `ysu.json`: å¾®è°ƒæ•°æ®é›†
- `download.py`: æ¨¡å‹ä¸‹è½½è„šæœ¬
- `README.md`: é¡¹ç›®è¯´æ˜

## ğŸ› ï¸ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå®‰è£…
æ¨èä½¿ç”¨ `uv` è¿›è¡ŒåŒ…ç®¡ç†ï¼š
\`\`\`bash
source vllm_env/bin/activate
pip install -r requirements.txt
\`\`\`

### 2. ä¸‹è½½æ¨¡å‹
è¿è¡Œè„šæœ¬ä» ModelScope ä¸‹è½½ ChatGLM3-6Bï¼š
\`\`\`bash
python download.py
\`\`\`

### 3. å¯åŠ¨æœåŠ¡
é¦–å…ˆå¯åŠ¨åç«¯ APIï¼š
\`\`\`bash
CUDA_VISIBLE_DEVICES=0 API_PORT=8000 llamafactory-cli api \
    --model_name_or_path /root/autodl-tmp/ysu_merged_model \
    --template chatglm3 \
    --finetuning_type full \
    --infer_backend huggingface
\`\`\`

ç„¶åå¯åŠ¨å‰ç«¯é¡µé¢ï¼š
\`\`\`bash
streamlit run app.py --server.port 6006
\`\`\`

## ğŸ“Š å¾®è°ƒç»†èŠ‚
- **åŸºåº§æ¨¡å‹**: ChatGLM3-6B
- **å¾®è°ƒæ–¹å¼**: LoRA
- **æ•°æ®é›†**: ysu.json (ç‡•å¤§æ•™åŠ¡é—®ç­”æ•°æ®)

