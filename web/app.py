import streamlit as st
from openai import OpenAI
import os

# --- é¡µé¢åŸºç¡€é…ç½® ---
st.set_page_config(
    page_title="YSU æ•™åŠ¡é—®ç­”åŠ©æ‰‹ (AutoDLç‰ˆ)",
    page_icon="ğŸ“",
    layout="wide"
)

# --- ä¾§è¾¹æ é…ç½® ---
with st.sidebar:
    st.title("âš™ï¸ æ¨¡å‹é…ç½®")
    st.info("åç«¯è¿æ¥: AutoDL æœ¬åœ° API")
    
    api_host = st.text_input("API åœ°å€", "http://localhost:8000/v1")
    model_name = st.text_input("æ¨¡å‹åç§°", "chatglm3")
    
    temperature = st.slider("éšæœºæ€§", 0.0, 1.0, 0.1)
    max_tokens = st.slider("å›å¤é•¿åº¦", 128, 4096, 2048)
    # å¼ºåˆ¶å…³é—­æµå¼è¾“å‡ºå¼€å…³ï¼ˆç”¨äºè°ƒè¯•ï¼‰
    use_stream = st.toggle("å¼€å¯æµå¼è¾“å‡º (Stream)", value=False)
    
    if st.button("æ¸…é™¤å¯¹è¯å†å²"):
        st.session_state.messages = []
        st.rerun()

# --- èŠå¤©æ ¸å¿ƒé€»è¾‘ ---
st.title("ğŸ“ ç‡•å±±å¤§å­¦æ•™åŠ¡æ™ºèƒ½é—®ç­”")
st.caption("åŸºäº ChatGLM3-6B + YSUå¾®è°ƒæ¨¡å‹")

client = OpenAI(api_key="0", base_url=api_host)

# åˆå§‹åŒ–å†å²è®°å½•ï¼ˆåŒ…å«æ¬¢è¿è¯­ï¼‰
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "assistant", "content": "åŒå­¦ä½ å¥½ï¼æˆ‘æ˜¯ç‡•å±±å¤§å­¦æ•™åŠ¡åŠ©æ‰‹ï¼Œæœ‰ä»€ä¹ˆå…³äºé€‰è¯¾ã€è€ƒè¯•æˆ–è½¬ä¸“ä¸šçš„é—®é¢˜å¯ä»¥é—®æˆ‘ã€‚"}
    ]

# æ¸²æŸ“ç•Œé¢ï¼ˆä¿ç•™æ¬¢è¿è¯­ï¼‰
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# å¤„ç†ç”¨æˆ·è¾“å…¥
if prompt := st.chat_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        
        # --- ã€å…³é”®ä¿®å¤ã€‘æ„å»ºå‘é€ç»™ API çš„æ¶ˆæ¯åˆ—è¡¨ ---
        # è¿‡æ»¤æ‰ç¬¬ä¸€æ¡æ¬¢è¿è¯­ï¼Œç¡®ä¿å¯¹è¯ä»¥ User å¼€å¤´
        api_messages = []
        for msg in st.session_state.messages:
            if msg["content"] == "åŒå­¦ä½ å¥½ï¼æˆ‘æ˜¯ç‡•å±±å¤§å­¦æ•™åŠ¡åŠ©æ‰‹ï¼Œæœ‰ä»€ä¹ˆå…³äºé€‰è¯¾ã€è€ƒè¯•æˆ–è½¬ä¸“ä¸šçš„é—®é¢˜å¯ä»¥é—®æˆ‘ã€‚":
                continue
            api_messages.append(msg)
        # -----------------------------------------

        try:
            if use_stream:
                stream = client.chat.completions.create(
                    model=model_name,
                    messages=api_messages, # ä½¿ç”¨è¿‡æ»¤åçš„æ¶ˆæ¯åˆ—è¡¨
                    temperature=temperature,
                    max_tokens=max_tokens,
                    stream=True
                )
                for chunk in stream:
                    if chunk.choices[0].delta.content is not None:
                        content = chunk.choices[0].delta.content
                        full_response += content
                        message_placeholder.markdown(full_response + "â–Œ")
                message_placeholder.markdown(full_response)
            else:
                # éæµå¼è¯·æ±‚ (æœ€ç¨³å¦¥æ¨¡å¼)
                with st.spinner("æ­£åœ¨æ€è€ƒä¸­..."):
                    response = client.chat.completions.create(
                        model=model_name,
                        messages=api_messages, # ä½¿ç”¨è¿‡æ»¤åçš„æ¶ˆæ¯åˆ—è¡¨
                        temperature=temperature,
                        max_tokens=max_tokens,
                        stream=False
                    )
                    full_response = response.choices[0].message.content
                    message_placeholder.markdown(full_response)
            
            st.session_state.messages.append({"role": "assistant", "content": full_response})
            
        except Exception as e:
            st.error(f"è¯·æ±‚å¤±è´¥: {e}")
